# -*- coding: utf-8 -*-
"""DCGAN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xFF8r7Ke7NcJZfo4XR-ci75tzPrgvxCZ
"""

#from mounted drive we unzip the image files
from zipfile import ZipFile
file_name = '/content/drive/MyDrive/Thesis datasets/Train run 3- [6000].zip' # give file path
with ZipFile(file_name, 'r') as zip:
  zip.extractall()
  print('Done')

# Commented out IPython magic to ensure Python compatibility.
#loading the tensorboard for display at the end
# %load_ext tensorboard

# Discriminator and Generator
import torch
import torch.nn as nn

class Discriminator(nn.Module):
  def __init__(self, channels_img, features_d):
    super(Discriminator, self).__init__()
    self.disc = nn.Sequential(
        # Input: N x channels_img x 64 x 64 # change to variables.
        nn.Conv2d(channels_img, features_d, 4, stride=2, padding=1, bias=False),#32x32
        nn.LeakyReLU(negative_slope=0.2, inplace=True),
        
        nn.Conv2d(features_d, features_d*2, 4, stride=2, padding=1, bias=False),#16x16
        nn.BatchNorm2d(features_d*2),
        nn.LeakyReLU(negative_slope=0.2, inplace=True),
           
        nn.Conv2d(features_d*2, features_d*4, 4, stride=2, padding=1, bias=False),#8x8
        nn.BatchNorm2d(features_d*4),
        nn.LeakyReLU(negative_slope=0.2, inplace=True),
             
        nn.Conv2d(features_d*4, features_d*8, 4, stride=2, padding=1, bias=False),#4x4
        nn.BatchNorm2d(features_d*8),
        nn.LeakyReLU(negative_slope=0.2, inplace=True),
        
        nn.Conv2d(features_d*8, 1, 4, stride=1, padding=0, bias=False),#1x1
        nn.Sigmoid()
    )
  
  def forward(self, x):
    return self.disc(x)

class Generator(nn.Module):
  def __init__(self, z_dim, channels_img, features_g):
    super(Generator, self).__init__()
    self.gen = nn.Sequential(
        # Input: N x z_dim x 1 x1 
        nn.ConvTranspose2d(z_dim, features_g*8, 4, stride=1, padding=0, bias=False),#8x8
        nn.BatchNorm2d(features_g*8),
        nn.ReLU(True),

        nn.ConvTranspose2d(features_g*8, features_g*4, 4, stride=2, padding=1, bias=False),#16x16
        nn.BatchNorm2d(features_g*4),
        nn.ReLU(True),

        nn.ConvTranspose2d(features_g*4, features_g*2, 4, stride=2, padding=1, bias=False),#32x32
        nn.BatchNorm2d(features_g*2),
        nn.ReLU(True),
        
        nn.ConvTranspose2d(features_g*2, features_g, 4, stride=2, padding=1, bias=False),#64x64
        nn.BatchNorm2d(features_g),
        nn.ReLU(True),

        nn.ConvTranspose2d(features_g,channels_img, 4, stride=2, padding=1, bias=False),#128x128
        nn.Tanh() #[-1,1]
    )

  def forward(self, x):
    return self.gen(x)

def initialize_weights(model):
  for m in model.modules():
    if isinstance(m, nn.Conv2d):
      nn.init.normal_(m.weight.data, 0.0, 0.02)
    elif isinstance(m, nn.BatchNorm2d):
      nn.init.normal_(m.weight.data, 1.0, 0.02)
      nn.init.constant_(m.bias.data, 0)

# Training
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from torchvision.transforms.transforms import CenterCrop
from torch.utils.data import DataLoader
from torch.utils.tensorboard import SummaryWriter
from sklearn.metrics import accuracy_score

#Losses and Scores lists
G_Loss = []
D_Loss = []
real_scores = []
fake_scores = []
the_accuracy_score = []

#The dataset path
the_root = "/content/Train run 3- [6000]"

# Hyperparameters etc. //no need change except last
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
LEARNING_RATE = 2e-4
Beta1 = 0.5
Beta2 = 0.999

# Hyperparameters etc. ///All changable data
BATCH_SIZE = 15 #will only take Int value
IMAGE_SIZE = 64 #won't work below 64
CHANNELS_IMG = 3
NOISE_DIMENSION = 100
NUM_EPOCHS = 40
FEATURES_DISCRIMINATOR = 64
FEATURES_GENERATOR = 64

#transforming the data
transforms = transforms.Compose(
    [
      transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),
      transforms.CenterCrop((IMAGE_SIZE, IMAGE_SIZE)),
      transforms.ToTensor(),
      transforms.Normalize(
          [0.5 for _ in range(CHANNELS_IMG)], [0.5 for _ in range(CHANNELS_IMG)]
      ),
    ]
)

#dataset we are using currently
dataset = datasets.ImageFolder(root = the_root,  transform= transforms)
dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle = True, num_workers = 2)

#generator and discriminator
gen = Generator(NOISE_DIMENSION, CHANNELS_IMG, FEATURES_GENERATOR).to(device)
disc = Discriminator(CHANNELS_IMG, FEATURES_DISCRIMINATOR).to(device)

#intializing weights
initialize_weights(gen)
initialize_weights(disc)

#optimizing using Adam optimizer for both and using BCELoss for criterion
opt_gen = optim.Adam(gen.parameters(), lr=LEARNING_RATE, betas=(Beta1, Beta2))
opt_disc = optim.Adam(disc.parameters(), lr=LEARNING_RATE, betas=(Beta1, Beta2))
criterion =nn.BCELoss()

#fixing noise and writers for fake and real and step=0
fixed_noise = torch.randn(64, NOISE_DIMENSION, 1, 1).to(device)
writer_real = SummaryWriter(f"logs/real")
writer_fake = SummaryWriter(f"logs/fake")
step = 0

#Training both
gen.train()
disc.train()

#epochs
for epoch in range(NUM_EPOCHS):
  for batch_idx, (real, _) in enumerate(dataloader):
    real = real.to(device)
    noise = torch.randn((BATCH_SIZE, NOISE_DIMENSION, 1, 1)).to(device)
    fake = gen(noise)

    ###Train Discriminator max log(D(X)) + log(1-D(G(z)))
    disc_real = disc(real).reshape(-1)
    loss_disc_real = criterion(disc_real, torch.ones_like(disc_real)) #crit(real_predictions, real_targets)
    real_score = torch.mean(disc_real)
    disc_fake = disc(fake.detach()).reshape(-1)
    loss_disc_fake = criterion(disc_fake, torch.zeros_like(disc_fake)) #crit(fake_predictions, fake_targets)
    fake_score = torch.mean(disc_fake)
    loss_disc = (loss_disc_real + loss_disc_fake) / 2
    disc.zero_grad()
    loss_disc.backward(retain_graph=True)
    opt_disc.step()

    ###Train Generator min log(1 - D(G(z))) <--> max log (D(G(z)))
    output = disc(fake).reshape(-1)
    loss_gen = criterion(output, torch.ones_like(output))
    gen.zero_grad()
    loss_gen.backward()
    opt_gen.step()

    # Print losses occasionally and print to tensorboard
    if batch_idx % 100 == 0:
        print(
          f"Epoch [{epoch}/{NUM_EPOCHS}] Batch {batch_idx}/{len(dataloader)} \
            Loss D: {loss_disc:.4f}, loss G: {loss_gen:.4f}"
        )
        G_Loss.append(loss_gen.item())
        D_Loss.append(loss_disc.item())
        real_scores.append(round(real_score.item()))
        fake_scores.append(round(fake_score.item()))
        the_accuracy_score.append(accuracy_score(real_scores,fake_scores).item())
        with torch.no_grad():
          fake = gen(fixed_noise)
          #take put (up to) Grid_Limit examples
          img_grid_real = torchvision.utils.make_grid(
              real[:32], normalize=True
          )
          img_grid_fake = torchvision.utils.make_grid(
              fake[:32], normalize=True
          )
          writer_real.add_image("Real", img_grid_real, global_step=step)
          writer_fake.add_image("Fake", img_grid_fake, global_step=step)
        step += 1

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir logs

import numpy as np
import matplotlib.pyplot as plt

plt.figure(figsize=(10,5))
plt.title("Generator and Discremenator Losses during training")
plt.plot(G_Loss, label="G")
plt.plot(D_Loss, label="D")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()
plt.show()

plt.figure(figsize=(10,5))
plt.title("Accuracy Score during training")
plt.plot(the_accuracy_score, label="Accuracy", color = 'green')
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend()
plt.show()

print(real_scores)
print(fake_scores)
print(the_accuracy_score)